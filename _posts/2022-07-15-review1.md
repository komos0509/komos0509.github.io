---
title:  "State-of-the-art in artificial neural network applications : A survey"
excerpt: "ANN 논문"

categories:
  - review
tags:
  - [review]

toc: true
toc_sticky: true
toc_label : "C O N T E N T S"
 
date: 2022-07-16
last_modified_at: 2022-07-16
---  

# 출처
* Oludare Isaac Abiodun, Aman Jantan, Abiodun Esther Omolara, Kemi Victoria Dada, Nachaat AbdElatif Mohamed, Humaira Arshad. (2018). State-of-the-art in artificial neural network applications : A survey. SI: Computer Science. Heliyon. 1-24p
[링크](https://www.sciencedirect.com/science/article/pii/S2405844018332067)  
* 위키백과 [링크](https://ko.wikipedia.org/wiki/%ED%95%AD%EB%93%B1_%ED%95%A8%EC%88%98)
* review's blog [링크](https://reniew.github.io/12/)
* velog blog [링크](https://velog.io/@lighthouse97/%EA%B8%B0%EC%9A%B8%EA%B8%B0-%EC%86%8C%EC%8B%A4-%EB%AC%B8%EC%A0%9C%EC%99%80-ReLU-%ED%95%A8%EC%88%98)
* Erica Bae blog [링크](https://ericabae.medium.com/ml-softmax-%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4-%ED%95%A8%EC%88%98-7a8c0362b2a3)


본 논문의 내용은 ANN의 기본 개념과 여러 곳에서 적용되고, 다른 이론들보다 장점이 많다는 것을 어필합니다. 그래서 저는 일단 ANN의 기본 개념을 알아보도록 하겠습니다.  
이 논문 내용과 더불어 추가적인 내용은 제가 전에 공부했던 내용 + 구글링을 통해 입력했습니다.
<br></br><br></br>

# Artificail Neural Network (ANN)
## 기본개념  
사람은 뇌에 뉴런이라는 신경을 이용하여 행동을 결정하고, 사고를 하며, 판단합니다. 여기에서 영감을 얻어 이를 컴퓨터에서 실행하고자 인공지능(AI : Artificial Intelligence)을 만들었습니다. 인공지능의 다양한 기술 중 하나인 Neural Network는 바로 두뇌의 뉴런 신경을 컴퓨터에 옮긴거라고 보시면 이해하기 쉽습니다. NN은 스스로 학습을 하며 결론을 도출하는 딥러닝의 기술 중 하나입니다. 이런 NN를 적용할 때 여러가지 적용을 추가하여 여러가지 종류가 탄생했고 여기서 ANN은 딥러닝의 기본개념이라고 보시면 됩니다.  

<br></br>

## 구조 + 재료 
![ANN](https://user-images.githubusercontent.com/60602671/180800538-15f60b1d-505b-476e-a9e4-a58aefa6a1e6.png)
<cite>Artificial neural network architecture(Facundo Bre, Prediction of wind pressure coefficients on building surfaces using Artificial Neural Networks, ResearchGate, Nov 2017)</cite>   

기본적인 구조는 input layer, hidden layer, output layer 이렇게 3가지의 layer가 있습니다. 기본 적용은 input layer에서 데이터를 입력 받아 hidden layer에서 연산이 적용 되어 최종적으로 output layer에서 예측값이 도출됩니다. 그러면 참 값과 예측값의 차이를 계산하여 차이가 가장 적은 매개변수를 최적화합니다. 이렇게 하여 차이가 가장 적은 예측값을 구합니다.  

전체적인 틀은 이렇게 되어있고, 이제 여기에서 추가적으로 사용되는 재료들을 살펴보겠습니다.  
1. 노드(Node)  
노드는 데이터의 값이라고 생각하시면 편합니다. 그림에서는 원모양이 노드인데 각각 역할의 값들입니다.  

2. 가중치(Weight), 바이어스(Bias)
가중치와 바이어스는 hidden layer에서 입력의 특성을 파악하기 위해 가중치와 바이어스를 나타냅니다.

<br></br>

## 연산   
연산을 공부하시려면 행렬에 대한 지식이 필요합니다.  
일단 위 그림같은 구조를 선형연립방정식으로 표현합니다. 이해를 위해 input layer에 노드는 2개, hidden layer는 1개에다가 노드는 3개, 마지막 output layer에는 노드가 2개로 하겠습니다.  

![ANN2](https://user-images.githubusercontent.com/60602671/180828408-030dbdc4-593b-4d16-8f0e-8cb1d29175ca.PNG)  

$$
\begin{cases}
h1 = v_{11}x_1 + v_{12}x_2 + b_1\\
h2 = v_{21}x_1 + v_{22}x_2 + b_2\\
h3 = v_{31}x_1 + v_{32}x_2 + b_3
\end{cases} 　　

\begin{cases}
y1 = w_{11}h_1 + w_{12}h_2 + w_{13}h_3 + c_1\\
y2 = w_{21}h_1 + w_{22}h_2 + w_{23}h_3 + c_2
\end{cases} 　 (b_1, b_2, b_3, c_1, c_2는 바이어스)
$$
이 선형연립방정식을 행렬로 표현이 가능합니다.  
$$
\begin{pmatrix}h_1\\h_2\\h_3 \end{pmatrix}=\begin{pmatrix}v_{11}&v_{12}\\v_{21}&v_{22}\\v_{31}&v_{32} \end{pmatrix}\begin{pmatrix}x_1\\x_2 \end{pmatrix}+\begin{pmatrix}b_1\\b_2\\b_3 \end{pmatrix} 　　

\begin{pmatrix}y_1\\y_2 \end{pmatrix}=\begin{pmatrix}w_{11}&w_{12}&w_{13}\\w_{21}&w_{22}&w_{23} \end{pmatrix}\begin{pmatrix}h_1\\h_2\\h_3 \end{pmatrix}+\begin{pmatrix}c_1\\c_2 \end{pmatrix}
$$

<br></br>

## 활성화함수
위 연산은 선형방정식으로 이뤄져있습니다. 하지만 세상에서의 모든 문제는 꼭 선형방정식으로 이뤄진게 아닌 비선형방정식으로 이뤄진것이 많이 있습니다. 그래서 우리도 이 연산을 비선형방정식으로 바꿔주는것이 좋습니다. 그러기 위한 활성화함수가 필요합니다. 활성화 함수는 보통 위 행렬들의 값들을 구할 때 추가적으로 연산됩니다.
활성화 함수의 종류는 여러가지가 있습니다. 그 중 대표적인 것만 올리겠습니다.   

1. 선형함수(Linear Function) : $a(x) = ax + b$  
![ANN3](https://user-images.githubusercontent.com/60602671/180851098-49805e59-d28d-473c-904c-4b9db1f67434.png)
<br></br>

2. 시그모이드 함수(Sigmoid Function) : $\sigma(x) = \frac{1}{1+e^{-x}}$  
모든 입력값에 대해 0과 1 사이로 변환하는 역할. 0.5를 기준으로 두가지 클래스를 비교하는 이진분류 문제에 활용됩니다.
![ANN4](https://user-images.githubusercontent.com/60602671/180856415-3121d0d5-9fcc-4dc6-aa6e-d674fa83b0d3.png)
<br></br>

3. tanh 함수 (Tanh Function) : $a(x) = tanh(x)$  
시그모이드 함수와 유사합니다. 차이점은 -1 과 1 사이의 값을 취해 0과 음수값을 갖습니다. 또한 0 부근에서 시그모이드 함수보다 더 가파른 기울기를 갖습니다.  
![ANN5](https://user-images.githubusercontent.com/60602671/180856827-57832f23-9cab-4b4c-91cd-212af74e476c.png)  
<br></br>

4. ReLU 함수(Rectified Linear Unit Function) : $ReLU(x) = max(0, x)$  
두 개의 직선을 이어 만든 것으로 `비선형 함수`이지만 선형과 매우 유사한 성질을 가지고 있습니다.  
![ANN6](https://user-images.githubusercontent.com/60602671/180857583-c93c20a3-0547-46c5-aba8-783f526d3023.png) 
<br></br>

5. Leaky ReLU 함수 : $a(x) = \begin{cases}
x,\;if\;x>0\\
\alpha x,\;if\;x\le0
\end{cases}$  
마이너스 값도 취할 수 있는 ReLU입니다.  
![ANN7](https://user-images.githubusercontent.com/60602671/180858241-5422123e-57d7-47c7-81fe-0a77b2b913b1.png)
<br></br>

6. ELU 함수(Exponential Linear Unit Function) : $a(x) = \begin{cases}
x,\;if\;x>0\\
\alpha (e^x-1),\;if\;x\le0
\end{cases}$  
Leaky ReLU 보다 부드러운 함수입니다.  
![ANN8](https://user-images.githubusercontent.com/60602671/180858780-ced098dc-59d7-4a14-be9a-5dc9a5f0729c.png)  
<br></br>

7. 소프트맥스 함수(Softmax Function) : $X = (x_1, x_2, \cdots, x_n)에 대하여 softmax(x) = \frac{e^x}{\sum ^n_{k=1}e^{x_k}}$  
분류문제에 최적화된 함수입니다. 모든 성분들의 합은 항상 1이라서 각 성분을 확률처럼 사용할 수 있습니다.(모든 성분은 0 ~ 1 사이의 값입니다.)  
![ANN9](https://user-images.githubusercontent.com/60602671/180860284-2b674f91-9775-4e65-aa42-cf4aaa8d4e01.png)

<br></br>

## loss 함수 + 경사하강법 
