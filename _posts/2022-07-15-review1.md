---
title:  "ANN 논문 리뷰 1"
excerpt: "State-of-the-art in artificial neural network applications : A survey"

categories:
  - review
tags:
  - [review, ANN, DL]

toc: true
toc_sticky: true
toc_label : "C O N T E N T S"
 
date: 2022-07-16
last_modified_at: 2022-07-26
---  

# 출처
* Oludare Isaac Abiodun, Aman Jantan, Abiodun Esther Omolara, Kemi Victoria Dada, Nachaat AbdElatif Mohamed, Humaira Arshad. (2018). State-of-the-art in artificial neural network applications : A survey. SI: Computer Science. Heliyon. 1-24p
[링크](https://www.sciencedirect.com/science/article/pii/S2405844018332067)  
* 위키백과 [링크](https://ko.wikipedia.org/wiki/%ED%95%AD%EB%93%B1_%ED%95%A8%EC%88%98)
* review's blog [링크](https://reniew.github.io/12/)
* velog blog [링크](https://velog.io/@lighthouse97/%EA%B8%B0%EC%9A%B8%EA%B8%B0-%EC%86%8C%EC%8B%A4-%EB%AC%B8%EC%A0%9C%EC%99%80-ReLU-%ED%95%A8%EC%88%98)
* Erica Bae blog [링크](https://ericabae.medium.com/ml-softmax-%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4-%ED%95%A8%EC%88%98-7a8c0362b2a3)


본 논문의 내용은 ANN의 기본 개념과 여러 곳에서 적용되고, 다른 이론들보다 장점이 많다는 것을 어필합니다. 그래서 저는 일단 ANN의 기본 개념을 알아보도록 하겠습니다.  
이 논문 내용과 더불어 추가적인 내용은 제가 전에 공부했던 내용 + 구글링을 통해 입력했습니다.
<br/><br/>

# Artificail Neural Network (ANN)
## 기본개념  
사람은 뇌에 뉴런이라는 신경을 이용하여 행동을 결정하고, 사고를 하며, 판단합니다. 여기에서 영감을 얻어 이를 컴퓨터에서 실행하고자 인공지능(AI : Artificial Intelligence)을 만들었습니다. 인공지능의 다양한 기술 중 하나인 Neural Network는 바로 두뇌의 뉴런 신경을 컴퓨터에 옮긴거라고 보시면 이해하기 쉽습니다. NN은 스스로 학습을 하며 결론을 도출하는 딥러닝의 기술 중 하나입니다. 종류로는 정답지가 주어진 지도학습, 정답지가 주어지지않은 비지도학습이 있습니다. 이 부분은 따로 포스팅 하겠습니다. 이런 NN를 적용할 때 여러가지 적용을 추가하여 여러가지 종류가 탄생했고 여기서 ANN은 딥러닝의 기본개념이라고 보시면 됩니다.  

<br/>

## 구조 + 재료 
![ANN](https://user-images.githubusercontent.com/60602671/180800538-15f60b1d-505b-476e-a9e4-a58aefa6a1e6.png)
<cite>Artificial neural network architecture(Facundo Bre, Prediction of wind pressure coefficients on building surfaces using Artificial Neural Networks, ResearchGate, Nov 2017)</cite>   

기본적인 구조는 input layer, hidden layer, output layer 이렇게 3가지의 layer가 있습니다. 기본 적용은 input layer에서 데이터를 입력 받아 hidden layer에서 연산이 적용 되어 최종적으로 output layer에서 예측값이 도출됩니다. 그러면 참 값과 예측값의 차이를 계산하여 차이가 가장 적은 매개변수를 최적화합니다. 이렇게 하여 차이가 가장 적은 예측값을 구합니다.  

전체적인 틀은 이렇게 되어있고, 이제 여기에서 추가적으로 사용되는 재료들을 살펴보겠습니다.  
1. 노드(Node)  
노드는 데이터의 값이라고 생각하시면 편합니다. 그림에서는 원모양이 노드인데 각각 역할의 값들입니다.  

2. 가중치(Weight), 바이어스(Bias)
가중치와 바이어스는 hidden layer에서 입력의 특성을 파악하기 위해 가중치와 바이어스를 나타냅니다.

<br/>

## 연산   
연산을 공부하시려면 행렬에 대한 지식이 필요합니다.  
일단 위 그림같은 구조를 선형연립방정식으로 표현합니다. 이해를 위해 input layer에 노드는 2개, hidden layer는 1개에다가 노드는 3개, 마지막 output layer에는 노드가 2개로 하겠습니다.  

![ANN2](https://user-images.githubusercontent.com/60602671/180828408-030dbdc4-593b-4d16-8f0e-8cb1d29175ca.PNG)  

$$
\begin{cases}
h1 = v_{11}x_1 + v_{12}x_2 + b_1\\
h2 = v_{21}x_1 + v_{22}x_2 + b_2\\
h3 = v_{31}x_1 + v_{32}x_2 + b_3
\end{cases} 　　

\begin{cases}
y1 = w_{11}h_1 + w_{12}h_2 + w_{13}h_3 + c_1\\
y2 = w_{21}h_1 + w_{22}h_2 + w_{23}h_3 + c_2
\end{cases} 　 (b_1, b_2, b_3, c_1, c_2는 바이어스)
$$

이 선형연립방정식을 행렬로 표현이 가능합니다.  

$$
\begin{pmatrix}h_1\\h_2\\h_3 \end{pmatrix}=\begin{pmatrix}v_{11}&v_{12}\\v_{21}&v_{22}\\v_{31}&v_{32} \end{pmatrix}\begin{pmatrix}x_1\\x_2 \end{pmatrix}+\begin{pmatrix}b_1\\b_2\\b_3 \end{pmatrix} 　　

\begin{pmatrix}y_1\\y_2 \end{pmatrix}=\begin{pmatrix}w_{11}&w_{12}&w_{13}\\w_{21}&w_{22}&w_{23} \end{pmatrix}\begin{pmatrix}h_1\\h_2\\h_3 \end{pmatrix}+\begin{pmatrix}c_1\\c_2 \end{pmatrix}
$$

<br/>

## 활성화함수
위 연산은 선형방정식으로 이뤄져있습니다. 하지만 세상에서의 모든 문제는 꼭 선형방정식으로 이뤄진게 아닌 비선형방정식으로 이뤄진것이 많이 있습니다. 그래서 우리도 이 연산을 비선형방정식으로 바꿔주는것이 좋습니다. 그러기 위한 활성화함수가 필요합니다. 활성화 함수는 보통 위 행렬들의 값들을 구할 때 추가적으로 연산됩니다.
활성화 함수의 종류는 여러가지가 있습니다. 그 중 대표적인 것만 올리겠습니다.   

1. 선형함수(Linear Function) : $a(x) = ax + b$  
![ANN3](https://user-images.githubusercontent.com/60602671/180851098-49805e59-d28d-473c-904c-4b9db1f67434.png)
<br/>

2. 시그모이드 함수(Sigmoid Function) : $\sigma(x) = \frac{1}{1+e^{-x}}$  
모든 입력값에 대해 0과 1 사이로 변환하는 역할. 0.5를 기준으로 두가지 클래스를 비교하는 이진분류 문제에 활용됩니다.
![ANN4](https://user-images.githubusercontent.com/60602671/180856415-3121d0d5-9fcc-4dc6-aa6e-d674fa83b0d3.png)
<br/>

3. tanh 함수 (Tanh Function) : $a(x) = tanh(x)$  
시그모이드 함수와 유사합니다. 차이점은 -1 과 1 사이의 값을 취해 0과 음수값을 갖습니다. 또한 0 부근에서 시그모이드 함수보다 더 가파른 기울기를 갖습니다.  
![ANN5](https://user-images.githubusercontent.com/60602671/180856827-57832f23-9cab-4b4c-91cd-212af74e476c.png)  
<br/>

4. ReLU 함수(Rectified Linear Unit Function) : $ReLU(x) = max(0, x)$  
두 개의 직선을 이어 만든 것으로 `비선형 함수`이지만 선형과 매우 유사한 성질을 가지고 있습니다.  
![ANN6](https://user-images.githubusercontent.com/60602671/180857583-c93c20a3-0547-46c5-aba8-783f526d3023.png) 
<br/>

5. Leaky ReLU 함수 : $a(x) = \begin{cases}
x,\;if\;x>0\\
\alpha x,\;if\;x\le0
\end{cases}$  
마이너스 값도 취할 수 있는 ReLU입니다.  
![ANN7](https://user-images.githubusercontent.com/60602671/180858241-5422123e-57d7-47c7-81fe-0a77b2b913b1.png)
<br/>

6. ELU 함수(Exponential Linear Unit Function) : $a(x) = \begin{cases}
x,\;if\;x>0\\
\alpha (e^x-1),\;if\;x\le0
\end{cases}$  
Leaky ReLU 보다 부드러운 함수입니다.  
![ANN8](https://user-images.githubusercontent.com/60602671/180858780-ced098dc-59d7-4a14-be9a-5dc9a5f0729c.png)  
<br/>

7. 소프트맥스 함수(Softmax Function) : $X = (x_1, x_2, \cdots, x_n)에 대하여 softmax(x) = \frac{e^x}{\sum ^n_{k=1}e^{x_k}}$  
분류문제에 최적화된 함수입니다. 모든 성분들의 합은 항상 1이라서 각 성분을 확률처럼 사용할 수 있습니다.(모든 성분은 0 ~ 1 사이의 값입니다.)  
![ANN9](https://user-images.githubusercontent.com/60602671/180860284-2b674f91-9775-4e65-aa42-cf4aaa8d4e01.png)

<br/>

## loss 함수
loss 함수는 오차를 구하는 함수이다. loss 함수의 종류로는 다양하지만 여기서는 절대오차를 구하는 것만 알아보겠습니다. 다양한 loss 함수는 나중에 최적화 알고리즘과 함께 같이 소개하겠습니다. 

* 절대 오차(Absolute error) : $\vert \hat{y} - y\vert$  
예측값 $\hat{y}$과 참 값 $y$의 차를 절댓값으로 감싼 것으로 예측값과 참값의 차이가 절대적으로 얼마나 차이가 나는지를 알 수 있습니다.

우리의 목표는 이 loss 함수값을 허용 오차까지 최소화하는 것이 목표입니다.
<br/>


## 경사하강법 
우리는 loss 함수로 얻은 오차를 최소화 하는것을 목표로 하고 있습니다. 그래서 우리는 경사하강법을 이용하여 매개변수를 최적화 시키고 다시 예측을하여 새로운 loss를 구합니다.  
이 포스팅에서는 경사하강법을 소개하겠습니다. 다양한 방법들은 추후 포스팅하겠습니다.  

### 하강법(Descent Method)  
$w  \leftarrow w + \mu \Delta w, 　　\nabla f(w)^T \Delta w < 0$ 　　 ($\mu$ : 학습률, $\Delta w$ : 탐색방향, $w$ : 매개변수)   
탐색 방법 $\Delta w$에 따라 하강법이 정해집니다. (경사하강법, 뉴턴방법)  

### 경사하강법(Gradient Descnet Method)
경사하강법은 1차 근삿값 발견용 최적화 알고리즘입니다. 기본 개념은 함수의 기울기를 구하고 경사의 반대 방향으로 계속 이동시켜 극값에 이룰 때까지 반복시키는 것입니다.  

$w \leftarrow w + \mu \Delta w, 　\nabla f(w)^T \Delta w < 0$ 에서 　 $ \Delta w = -\nabla f(w) $ 이라고 가정합시다. 그렇다면

$$ \nabla f(w)^T \Delta w = \nabla f(w)^T(-\nabla f(w)) = -\nabla f(w)^T\nabla f(w) < 0  이 되고, w \leftarrow w + \mu \Delta w = w  \leftarrow w - \mu \nabla f(w)
$$

즉 함수 $f(x)$는 위 연산에서 선형연립방정식이 되고, $w$는 최적화할 매개변수가 됩니다. [연산](#연산) 그리하여 우리는 기울기를 계산하기 위해 $f(x)$의 미분식을 알아야 합니다. 그리고 하기 전 학습률$(\mu)$, 초기 가중값$(V, W)$을 임의의 값으로 설정하고, 학습 횟수$(n)$ 및 허용오차$(tol)$를 설정해야합니다. [연산](#연산)의 식을 가져와봅시다.(바이어스는 미분을 하면 없어지기 때문에 넣지 않겠습니다.)

<br/>

$$
h^{hidden} = V^{(n)}x \rightarrow z = \phi_{hidden}(h^{hidden}) \rightarrow h^{out} = W^{(n)}z \rightarrow y = \phi_{out}(h^{out})
$$

여기서 $h^{hidden}, h^{out}$은 각각 연산되는 노드 값이라고 보시면 됩니다. $V^{(n)}, W^{(n)}$은 가중치입니다. 여기서 지수는 횟수를 표시하기 위한 장치입니다. $\phi_{hidden}, \phi_{out}$은 활성화 함수입니다. 여기서 경사하강법을 적용시켜 보겠습니다.  

$$
\frac{\partial y}{\partial W} = \frac{\partial \phi_{out}(h^{out})}{\partial W} = \phi^{'}_{out}(h^{out})\frac{\partial h^{out}}{\partial W} = \phi^{'}_{out}(h^{out})z
$$

$$
\frac{\partial y}{\partial V} = \frac{\partial \phi_{out}(h^{out})}{\partial V} = \phi^{'}_{out}(h^{out})\frac{\partial h^{out}}{\partial V}=\phi^{'}_{out}(h^{out})\frac{\partial W^{(n)}z}{\partial V}
$$

$$
=\phi^{'}_{out}(h^{out})W^{(n)}\frac{\partial z}{\partial V}=\phi^{'}_{out}(h^{out})W^{(n)}\frac{\partial \phi_{hidden}(h^{hidden})}{\partial V}
$$

$$
=\phi^{'}_{out}(h^{out})W^{(n)}\phi^{'}_{hidden}(h^{hidden})\frac{\partial h^{hidden}}{\partial V}=\phi^{'}_{out}(h^{out})W^{(n)}\phi^{'}_{hidden}(h^{hidden})x
$$

이렇게 기울기를 얻었고 경사하강법 식을 가져오면 아래와 같습니다.
$$
w^{(n+1)}=w^{(n)}-\mu\frac{\partial y}{\partial W}, 　　v^{(n+1)}=v^{(n)}-\mu\frac{\partial y}{\partial V}
$$

이로써 매개변수 $V, W$를 업데이트시켜서 최적화를 진행하면 됩니다.

# 추가적으로 필요한 것  
* 코드 구현
* 머신러닝 종류 (지도학습, 비지도학습, $\cdots$)
* loss 함수 종류와 최적화 알고리즘 종류


