---
title:  "ANN 논문 리뷰 2"
excerpt: "Introduction to Artificial Neural Network"

categories:
  - review
tags:
  - [review, ANN, DL]

toc: true
toc_sticky: true
toc_label : "C O N T E N T S"
 
date: 2022-07-26
last_modified_at: 2022-08-03
---  

# 출처
* A.D.Dongare, R.R.Kharde, Amit D.Kachare. (1, July 2012). Introduction to Artificial Neural Network. International Journal of Engineering and Innovative Technology (IJEIT). ISSN: 2277-3754 [PDF](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1082.1323&rep=rep1&type=pdf)

이 논문은 전반적인  ANN의 내용이 담겨져있습니다. 저는 여기서 ANN 연산을 위한 수식과 오차를 구하는 식, ANN의 종류를 알아보도록 하겠습니다.

<br/><br/>

# ANN 문학적 구조
1. 지도 학습(Supervised Learning)
지도학습은 입력(input)과 출력(output)의 패턴을 조화롭게 규정하여 네트워크를 훈련시킵니다. 이 입력과 출력은 외부의 정답을 이용하여 규정합니다.  

2. 비지도 학습(Unsupervised Learning)
비지도학습은 입력 범위 내 패턴의 군집의 응답하여 훈렵합니다. 외부의 정답 없이 비슷한 입력들을 비교하여 학습합니다.  

3. 보강한 학습(Reinforced Learning)
보강한 학습은 지도학습과 비지도학습의 중간 지점이라고 생각하시면 됩니다. 이 학습은 작업환경에서 피드백을 받은 뒤 다시 학습을 합니다. 이 시스템은 매개변수들을 조정하여 좋고 나쁨의 등급만 있습니다. 매개변수들은 평행상태가 될 때 까지 반복하여 조정됩니다. 

<br/><br/>

# ANN 연산
생물학적 뉴런을 함수로 만든다면, 3가지 중요한 기본 요소가 있습니다. 첫번째는 가중치입니다. 가중치는 입력(input)과 출력(output) 사이의 변수 크기입니다. 가중치가 마이너스이면 연결이 약해지고, 플러스이면 연결을 더욱 강해집니다. 나머지 두개는 뉴런 셀에서 활동하여 가중치를 수정하고, 수정된 각각의 가중치를 더합니다.
마지막으로 활성화 함수는 뉴런의 출력(output)의 범위를 조절합니다. 허용되는 범위는 대체로 0과 1사이 또는 -1과 1사이입니다.

![ANN10](https://user-images.githubusercontent.com/60602671/182395163-d65ef322-0ce5-4c2d-9562-5cf928240da5.PNG)  
<cite>Mathematical Model(A.D.Dongare, R.R.Kharde, Amit D.Kachare.1, July 2012)Introduction to Artificial Neural Network. International Journal of Engineering and Innovative Technology (IJEIT). ISSN: 2277-3754</cite>  

레이어 사이의 연산을 수식으로 나타내면  $v_k = \displaystyle\sum_{j=1}^{p}{w_{kj}x_j}$ 입니다. 여기서 활성화 함수 $\theta_k$를 적용하면 됩니다.

<br/><br/>

# 성능함수
학습 중에 일어나는 문제 중 하나는 오버피팅(over fitting)입니다. 오버피팅은 학습데이터에서의 학습이 지나치게 잘되서 학습데이터에 대해서는 오류가 적지만, 다른 데이터에 대해서는 높은 오류가 나타나는 현삽입니다. 이 현상을 방지하기 위해서는 적절한 매개변수로 조정해야합니다. 전형적인 성능함수는 MSE(Mean Sum Error)입니다.  

$$
mse = \frac{1}{N}\displaystyle\sum_{i=1}^{N}(e_i)^2=\frac{1}{N}\displaystyle\sum_{i=1}^{N}(X_{real}(i)-X_{predicted}(i))^2
$$ 

이 성능함수를 이용하여 네트워크의 가중치와 바이어스를 구성하도록 수정하면 일반화를 할 수 있습니다. 하지만 더 좋은 성능함수가 있습니다.  

$$
msereg = \lambda mse + (1-\lambda)msw,\\
Where \lambda is the performace ratio, and\\
msw=\frac{1}{N}\displaystyle \sum_{j=1}^{N}w_{j}^2
$$

이 성능함수를 사용하면 비슷한 가중치와 바이어스를 얻게 되고, 네트워크의 응답을 부드럽고 오버피팅이 적게합니다. 